{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Superpowered AI is a Knowledge Base as a Service (KBaaS) that provides a robust and efficient way to create, store, and query knowledge bases. In this tutorial, we will demonstrate how the real-time query endpoint can be used to efficiently search a webpage for information relevant to a specific query with a single API call.\n",
    "\n",
    "Autonomous agents like AutoGPT can navigate the web and gather relevant information and perform actions. One challenge with building this kind of agent is that many webpages have too much text to fit into the context window of LLMs like GPT-3 and GPT-4. Agents can only see what's in their context window, so that's a problem. How can we expand access to things that don't fit in context? The answer is semantic search. \n",
    "\n",
    "Generally this is done by breaking the text into chunks, creating vector embeddings for each chunk, uploading those vectors to a vector database, and then querying the database with the query embedding vector. For an application like web browsing, where you may never need to search that exact page again, this is a pretty inefficient way to do it. We've built a more efficient and cost-effective way to do this called real-time query. You simply provide the query along with the content you want to search in a single API call, and then we return the most relevant text snippets, as well as an LLM-generated summary.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we start, ensure you have the following:\n",
    "\n",
    "1. A Superpowered AI account with API keys (Sign up [here](https://superpowered.ai) for free access).\n",
    "2. Python 3 installed on your computer.\n",
    "3. `beautifulsoup4` and `requests` libraries installed (you can install them using `pip install beautifulsoup4 requests`).\n",
    "\n",
    "## Step-by-Step Tutorial\n",
    "\n",
    "### Step 1: Set up the environment\n",
    "\n",
    "First, we will set up the environment by importing the required libraries and initializing the Superpowered AI SDK with our API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superpowered import real_time_query, init\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set API keys\n",
    "os.environ[\"SUPERPOWERED_API_KEY_ID\"] = \"YOUR_API_KEY_ID\"\n",
    "os.environ[\"SUPERPOWERED_API_KEY_SECRET\"] = \"YOUR_API_KEY_SECRET\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `\"YOUR_API_KEY_ID\"` and `\"YOUR_API_KEY_SECRET\"` with your actual API keys.\n",
    "\n",
    "### Step 2: Scrape the web page\n",
    "\n",
    "Next, we will scrape the content of a web page page using the `requests` library and `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Mont_Blanc\"\n",
    "\n",
    "# scrape the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "body = soup.find('body')\n",
    "content = body.text\n",
    "\n",
    "print (content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if that doesn't work for the web page you want to scrape, you can use Selenium, which is slower but works for a much wider variety of websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attempting to get title and content from url:  https://superpoweredai.notion.site\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/3h4y4h_j4tqc14cjkhsznls00000gn/T/ipykernel_12634/2748295580.py:13: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"/opt/chromedriver\", options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of content:  6602\n",
      "Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.\n",
      "Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.üíºSuperpowered AI DocumentationNote: this documentation is a work in progress, so we highly recommend you join the conversation on our Discord!MotivationLLMs have emerged as a groundbreaking new technology. But when used on their own they have some major shortcomings. For real-world use cases, you need to integrate additional functionality into them to overcome these issues. Access to external knowledge sources. LLMs have a vast amount of knowledge stored in their parameters, but that knowledge can be unreliable. They also don‚Äôt have any knowledge of private information, like your company‚Äôs internal documents. Giving LLMs access to knowledge sources at inference time solves both of these problems.Long-term memory. Want to build a chatbot that remembers users‚Äô preferences over long periods of time? You‚Äôll need to store conversations and retrieve them later on.The Embeddings + Vector Database ParadigmThe most common way to solve these problems is to take the text you want to store, break it up into small chunks, run an embedding model on each chunk to convert it to a vector, and then upload those vectors to a vector database. Then you can query the vector database against a user input to find the most similar pieces of text from the database. Finally, you include these text chunks in the LLM prompt to give it the context it needs.This basic model works pretty well, but there are improvements that can be made to it, both in performance and in user experience. Superpowered AI is a managed solution that does all of this for you, and then some. We call it a Knowledge Base as a Service, or KBaaS.Knowledge Base as a ServiceUltimately what you accomplish with the embeddings + vector database model is you create a place where you can dump your files and then query them. So we decided to just abstract that away for developers and directly give you a place to dump files and query from them, without having to deal with the messy stuff in the middle.Multi-stage retrieval pipelineSuperpowered AI offers improved query performance over more basic knowledge retrieval pipelines by using a multi-stage approach.Dense Vector Search (Stage 1)The first stage of the retrieval pipeline involves a dense vector search (i.e. embeddings). In this stage, the input query is converted into a dense vector representation using a pre-trained language model. Vector representations were calculated for all of the documents in the knowledge base on upload, so now we can compute the semantic similarity between the input query and the documents by simply measuring the distance between their embeddings vectors.The dense vector search allows for efficient retrieval of a list of top-k (usually 100-500) most relevant documents based on the similarity scores, even when searching over knowledge bases with millions of documents. This step significantly reduces the search space for the subsequent stages, ensuring faster and more accurate results.Cross Encoder (Stage 2)The second stage of the retrieval pipeline uses a cross encoder to rerank the results returned from the first stage retriever. The cross encoder is another pre-trained language model that takes the input query and the top-k documents retrieved from the first stage as input pairs. It then computes a relevance score for each pair, which is a measure of how well the document matches the query.The cross encoder is more computationally expensive than the dense vector search, but it provides a more accurate measure of relevance between the query and the documents. By using the cross encoder in the second stage, we ensure that our pipeline efficiently narrows down the search space before investing computational resources in this more accurate scoring method.Generative LLM (Optional Stage 3)The third stage of the retrieval pipeline is optional and involves a generative language model. This stage is used to extract and summarize the top results from the previous stages. The generative LLM takes the top-ranked documents from the cross encoder stage and generates a concise and coherent summary that addresses the input query.This stage is particularly useful for applications that require a quick and easily digestible response to a query, as it condenses the most relevant information into a single output.How to create and query from Knowledge BasesPlaygroundYou can create new Knowledge Bases, upload files, and query them directly from our Playground.APIThe documentation for Superpowered‚Äôs REST API can be found here:API DocumentationPython SDKSDK DocumentationExample use casesHere are some ideas for what you could build with Superpowered AI:Virtual employee Slack bot. Upload company documents like internal knowledge bases, employee handbooks, etc. to a Knowledge Base. Then build a Slack bot that your employees can talk to whenever they have questions about internal policies and best practices.Copywriting assistant w/ context. Create a Knowledge Base with detailed information about your company and products. Then create a copywriting tool that queries your Knowledge Base and puts relevant information into the prompt, and everything it writes will be customized to your company‚Äôs brand.Domain-specific ChatGPT. Create a Knowledge Base with a variety of documents on a certain topic and then create a simple chatbot, like ChatGPT. When users ask questions you can query your Knowledge Base to get relevant information to include in the prompt.Autonomous agent web browsing. A core capability of autonomous agents like AutoGPT is the ability to browse the web. Our Real Time Query endpoint is perfect for this use case. You can just grab the full text from each webpage the agent visits and feed it to the Real Time Query endpoint along with an agent-generated query. This is substantially cheaper than feeding the full text to GPT-3 or GPT-4.Long-term memory for chatbots. Create a Knowledge Base for each of your users, and then put their full chat histories in it. Query it after each user message to find relevant messages from their long-term chat history. LimitationsThere are a few things we‚Äôre not set up to handle just yet:High query volumes (more than ~10 per second)Very large knowledge bases (more than ~1B tokens)Very large file uploads (over ~5GB for a single file)PricingWe‚Äôre offering free access to Superpowered AI for a limited time. You can create an account here and get started creating your first Knowledge Base in minutes.Once we add billing, here is what our pricing will be:UploadStorageQueryProduct roadmap\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "url = \"https://superpoweredai.notion.site\"\n",
    "\n",
    "@contextmanager\n",
    "def get_chrome_driver(options):\n",
    "    \"\"\"\n",
    "    context manager to ensure `driver.quit()` is called after execution\n",
    "    to avoid memory leaks and zombie processes\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(\"/opt/chromedriver\", options=options)\n",
    "    try:\n",
    "        yield driver\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def get_title_and_content_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    get the human-readable text from the website\n",
    "\n",
    "    This will require us to first render the page using a headless browser\n",
    "    and then get the text from the page\n",
    "    \"\"\"\n",
    "\n",
    "    print('attempting to get title and content from url: ', url)\n",
    "\n",
    "    # Use a headless Chrome browser for rendering\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "\n",
    "    with get_chrome_driver(options) as browser:\n",
    "        browser.set_page_load_timeout(20)\n",
    "\n",
    "        # Navigate to the URL and wait for the page to render\n",
    "        browser.get(url)\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Get the HTML code of the page\n",
    "        html = browser.page_source\n",
    "\n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title = soup.title.string if soup.title is not None else url\n",
    "    content = soup.get_text()\n",
    "    print('length of content: ', len(content))\n",
    "\n",
    "    return title, content\n",
    "\n",
    "title, content = get_title_and_content_from_url(url)\n",
    "\n",
    "print (title)\n",
    "print (content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Initialize the Superpowered AI SDK\n",
    "\n",
    "Initialize the Superpowered AI SDK with the API keys you set in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(api_key_id=os.environ[\"SUPERPOWERED_API_KEY_ID\"], api_key_secret=os.environ[\"SUPERPOWERED_API_KEY_SECRET\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query the scraped content\n",
    "\n",
    "Now, we will query the scraped content using the real-time query endpoint. We will ask the question, \"What is the name of the highest mountain in the Alps?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the name of the highest mountain in the Alps?\"\n",
    "response = real_time_query(query=query, passages=[content], top_k=5, summarize_results=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `real_time_query()` function takes the following parameters:\n",
    "\n",
    "- `query`: The question we want to ask.\n",
    "- `passages`: A list containing the content we want to search.\n",
    "- `top_k`: The number of top results to return.\n",
    "- `summarize_results`: Whether to extract and summarize the top results.\n",
    "\n",
    "### Step 5: Print the results\n",
    "\n",
    "Finally, we will print the summarized response and the top results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mont Blanc [2][3] is the highest mountain in the Alps and Western Europe, rising 4,807.81 m (15,774 ft) above sea level, located on the French-Italian border. It is the second-most prominent mountain in Europe after Mount Elbrus, and the eleventh highest peak in the world. The mountain is part of the Graian Alps and is situated in the Aosta Valley, Italy, and Auvergne-Rh√¥ne-Alpes, France. The first ascent of Mont Blanc was made on 8 August 1786 by Jacques Balmat and Michel-Gabriel Paccard.\n"
     ]
    }
   ],
   "source": [
    "print (response[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,result in enumerate(response[\"results\"]):\n",
    "    print (f\"Result {i+1}:\")\n",
    "    print (result[\"content\"])\n",
    "    print ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44d0561a9d33f22b2e67e0485c48036e39d1c698628b030a9859974b559ff507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
