{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Webinar 2024-02-28`**\n",
    "\n",
    "[YouTube Recording](https://www.youtube.com/watch?v=r-BJha0niXM&list=PLePKvgYhNOFxg3O7gXfmxaKp0EWXEhvNC&index=12)\n",
    "\n",
    "# Web Search\n",
    "\n",
    "Define globals and requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import superpowered\n",
    "from pprint import pprint\n",
    "\n",
    "API_KEY_ID = ''\n",
    "API_KEY_SECRET = ''\n",
    "\n",
    "superpowered.set_api_key(API_KEY_ID, API_KEY_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Web Search Preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsp = superpowered.create_web_search_preset(\n",
    "    title='Web Scholar',\n",
    "    include_domains=['arxiv.org', 'sciencedirect.com'],\n",
    "    timeframe_days=365,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Endpoint\n",
    "\n",
    "We will just use the web search preset rather than overrides\n",
    "\n",
    "##### NOTE: this uses the new `json_response` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'The GitHub repositories provide valuable information on '\n",
      "           'benchmarking inference speed for deep learning models on various '\n",
      "           'platforms and architectures. They discuss techniques such as '\n",
      "           'pruning, quantization, and optimization to improve performance. '\n",
      "           'Different frameworks and platforms are compared based on their '\n",
      "           'compatibility with models, detection support, and speed '\n",
      "           'benchmarks. The repositories also mention specific tools and '\n",
      "           'libraries like SparseML for pruning and quantization, as well as '\n",
      "           'various inference engines optimized for different hardware '\n",
      "           'configurations. Overall, the repositories offer a comprehensive '\n",
      "           'overview of the latest research and techniques aimed at enhancing '\n",
      "           'inference time in deep learning.',\n",
      " 'question': 'What is the latest research on processing units to improve '\n",
      "             'inference time in deep learning?'}\n",
      "{'ranked_results': [],\n",
      " 'references': None,\n",
      " 'search_queries': None,\n",
      " 'summary': '\\n'\n",
      "            '{\\n'\n",
      "            '  \"question\": \"What is the latest research on processing units to '\n",
      "            'improve inference time in deep learning?\",\\n'\n",
      "            '  \"answer\": \"The GitHub repositories provide valuable information '\n",
      "            'on benchmarking inference speed for deep learning models on '\n",
      "            'various platforms and architectures. They discuss techniques such '\n",
      "            'as pruning, quantization, and optimization to improve '\n",
      "            'performance. Different frameworks and platforms are compared '\n",
      "            'based on their compatibility with models, detection support, and '\n",
      "            'speed benchmarks. The repositories also mention specific tools '\n",
      "            'and libraries like SparseML for pruning and quantization, as well '\n",
      "            'as various inference engines optimized for different hardware '\n",
      "            'configurations. Overall, the repositories offer a comprehensive '\n",
      "            'overview of the latest research and techniques aimed at enhancing '\n",
      "            'inference time in deep learning.\"\\n'\n",
      "            '}',\n",
      " 'web_search_queries': None,\n",
      " 'web_search_references': [1, 4],\n",
      " 'web_search_results': [{'content': 'git clone the DeepLabCut-live! repo: git '\n",
      "                                    'clone '\n",
      "                                    'https://github.com/DeepLabCut/DLC-inferencespeed-benchmark.git '\n",
      "                                    \"and run  ./reinstall.sh  to be sure it's \"\n",
      "                                    'properly installed. Run our benchmarking '\n",
      "                                    'script on your system (with our '\n",
      "                                    'data/model). Within the DeepLabCut-Live '\n",
      "                                    'directory you will find the following '\n",
      "                                    'structure: Then you can run (with '\n",
      "                                    'python3, pythonw on MacOS): This will '\n",
      "                                    'take some time, depending on your '\n",
      "                                    'internet connection and hardware. Note '\n",
      "                                    'that downloading, might take a few '\n",
      "                                    'minutes, as the multiple models & videos '\n",
      "                                    'comprise about 2,2 GB. Then 4 models will '\n",
      "                                    'be run on two videos for various video '\n",
      "                                    'sizes. To get you a sense, this takes '\n",
      "                                    'about 90 minutes on a Titan RTX. IF you '\n",
      "                                    'want to run the benchmark on a CPU or '\n",
      "                                    'slow hardware, you can also change the '\n",
      "                                    'number of frames, to 1000 in '\n",
      "                                    'https://github.com/DeepLabCut/DeepLabCut-live/blob/master/benchmarking/run_dlclive_benchmark.py#L24. '\n",
      "                                    'Please make a pull request here (i.e., '\n",
      "                                    'add the resulting file to your forked '\n",
      "                                    'repo under the _data folder--i.e., no '\n",
      "                                    'need to hand edit the file, we will '\n",
      "                                    'automatically convert your files into the '\n",
      "                                    'correct yaml file format), and create a '\n",
      "                                    'new pull request! ) or email us: '\n",
      "                                    'admin@deeplabcut.org if you have any '\n",
      "                                    'trouble!',\n",
      "                         'score': 0.045836060269178705,\n",
      "                         'title': 'GitHub - '\n",
      "                                  'DeepLabCut/DLC-inferencespeed-benchmark: A '\n",
      "                                  'database of inference speed benchmark '\n",
      "                                  'results on various platforms and '\n",
      "                                  'architectures',\n",
      "                         'url': 'https://github.com/DeepLabCut/DLC-inferencespeed-benchmark'},\n",
      "                        {'content': 'We prune the encoder part in the '\n",
      "                                    'semi-structured 4-block pattern up to 50% '\n",
      "                                    'sparsity. Assuming that the SparseML '\n",
      "                                    'library is installed, the bash script to '\n",
      "                                    'reproduce our pruning setup is as '\n",
      "                                    'follows:  CUDA_VISIBLE_DEVICES=0 python '\n",
      "                                    'src/sparseml/transformers/question_answering.py '\n",
      "                                    '\\\\  --distill_teacher '\n",
      "                                    'zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none '\n",
      "                                    '\\\\  --model_name_or_path '\n",
      "                                    'zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none '\n",
      "                                    '\\\\  --recipe '\n",
      "                                    'obert_mobilebert_compression_recipe.yaml '\n",
      "                                    '\\\\ And the '\n",
      "                                    'obert_mobilebert_compression_recipe.yaml '\n",
      "                                    'is as follows:  - '\n",
      "                                    'mobilebert.embeddings.position_embeddings.weight  '\n",
      "                                    '- '\n",
      "                                    'mobilebert.embeddings.token_type_embeddings.weight '\n",
      "                                    'Now that we have a 50% semi-structured '\n",
      "                                    'pruned 14 layers of the MobileBERT model, '\n",
      "                                    'we apply INT8 quantization-aware training '\n",
      "                                    '(QAT) on top of it to further improve the '\n",
      "                                    'performance, while keeping the pruning '\n",
      "                                    'mask fixed. Assuming that the SparseML '\n",
      "                                    'library is installed, the bash script to '\n",
      "                                    'reproduce our quantization setup is as '\n",
      "                                    'follows:  CUDA_VISIBLE_DEVICES=0 python '\n",
      "                                    'src/sparseml/transformers/question_answering.py '\n",
      "                                    '\\\\  --distill_teacher '\n",
      "                                    'zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none '\n",
      "                                    '\\\\  --model_name_or_path '\n",
      "                                    '/path/to/the/pruned/checkpoint/from/the/previous/step '\n",
      "                                    '\\\\  --recipe '\n",
      "                                    'obert_mobilebert_quantization_recipe.yaml '\n",
      "                                    '\\\\ And the '\n",
      "                                    'obert_mobilebert_quantization_recipe.yaml '\n",
      "                                    'is as follows:  submodules: '\n",
      "                                    \"['mobilebert.embeddings', \"\n",
      "                                    \"'mobilebert.encoder', 'qa_outputs']  \"\n",
      "                                    'Final Step: Export to ONNX and Benchmark '\n",
      "                                    'with DeepSparse To run the compressed and '\n",
      "                                    'quantized obert-mobilebert model in the '\n",
      "                                    'DeepSparse engine, we need to export it '\n",
      "                                    'to ONNX with:  --model_path '\n",
      "                                    '/path/to/my/compressed/and/quantized/model '\n",
      "                                    \"\\\\  --task 'question-answering' \"\n",
      "                                    '--sequence_length 384    '\n",
      "                                    'deepsparse.benchmark '\n",
      "                                    '/path/to/my/compressed/and/quantized/model.onnx '\n",
      "                                    'For more details about our compression '\n",
      "                                    'approach, please check the Optimal BERT '\n",
      "                                    'Surgeon (oBERT) paper: '\n",
      "                                    'https://arxiv.org/abs/2203.07259. For the '\n",
      "                                    'full algorithm implementation, more '\n",
      "                                    'recipes, examples and tutorials: '\n",
      "                                    'https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT. '\n",
      "                                    'If you find our models useful, please '\n",
      "                                    'consider citing our work:',\n",
      "                         'score': 0.015024304237158503,\n",
      "                         'title': 'mlperf_inference_results_v2.1/obert_mobilebert.md '\n",
      "                                  'at master · '\n",
      "                                  'neuralmagic/mlperf_inference_results_v2.1',\n",
      "                         'url': 'https://github.com/neuralmagic/mlperf_inference_results_v2.1/blob/master/open/NeuralMagic/obert_mobilebert.md'},\n",
      "                        {'content': ' This commit does not belong to any '\n",
      "                                    'branch on this repository, and may belong '\n",
      "                                    'to a fork outside of the repository.  A '\n",
      "                                    'tag already exists with the provided '\n",
      "                                    'branch name. Many Git commands accept '\n",
      "                                    'both tag and branch names, so creating '\n",
      "                                    'this branch may cause unexpected '\n",
      "                                    'behavior. Are you sure you want to create '\n",
      "                                    'this branch?  You signed in with another '\n",
      "                                    'tab or window. Reload to refresh your '\n",
      "                                    'session. You signed out in another tab or '\n",
      "                                    'window. Reload to refresh your session.',\n",
      "                         'score': 0.010221208201332141,\n",
      "                         'title': 'GitHub - mlcommons/inference_results_v2.0',\n",
      "                         'url': 'https://github.com/mlcommons/inference_results_v2.0'},\n",
      "                        {'content': '<p>This benchmark suite measures how fast '\n",
      "                                    'systems can process inputs and produce '\n",
      "                                    'results using a trained model. Below is a '\n",
      "                                    'short summary of the current benchmarks '\n",
      "                                    'and metrics. Please see the <a '\n",
      "                                    'href=\"https://arxiv.org/abs/1911.02549\">MLPerf '\n",
      "                                    'Inference benchmark paper</a> for a '\n",
      "                                    'detailed description of the motivation '\n",
      "                                    'and guiding principles behind the '\n",
      "                                    'benchmark suite.</p> <p>In order to '\n",
      "                                    'enable representative testing of a wide '\n",
      "                                    'variety of inference platforms and use '\n",
      "                                    'cases, MLPerf has defined four different '\n",
      "                                    'scenarios as described below. A given '\n",
      "                                    'scenario is evaluated by a standard load '\n",
      "                                    'generator generating inference requests '\n",
      "                                    'in a particular pattern and measuring a '\n",
      "                                    'specific metric.</p> <td>LoadGen sends '\n",
      "                                    'next query as soon as SUT completes the '\n",
      "                                    'previous query</td> <td>LoadGen sends a '\n",
      "                                    'new query every <em>latency '\n",
      "                                    'constraint</em> if the SUT has completed '\n",
      "                                    'the prior query, otherwise the new query '\n",
      "                                    'is dropped and is counted as one overtime '\n",
      "                                    'query</td> <td>Maximum number of '\n",
      "                                    'inferences per query supported</td> '\n",
      "                                    '<td>Loadgen sends next query, as soon as '\n",
      "                                    'SUT completes the previous query</td> '\n",
      "                                    '<td>LoadGen sends new queries to the SUT '\n",
      "                                    'according to a Poisson distribution</td> '\n",
      "                                    '<td>Maximum Poisson throughput parameter '\n",
      "                                    'supported</td> <td>LoadGen sends all '\n",
      "                                    'queries to the SUT at start</td> <p>Each '\n",
      "                                    'benchmark is defined by a Dataset and '\n",
      "                                    'Quality Target. The following table '\n",
      "                                    'summarizes the benchmarks in this version '\n",
      "                                    'of the suite (the rules remain the '\n",
      "                                    'official source of truth):</p>',\n",
      "                         'score': 0.0036553844488480536,\n",
      "                         'title': 'v1.1 Results',\n",
      "                         'url': 'https://mlcommons.org/en/inference-tiny-11/'},\n",
      "                        {'content': ' \\n'\n",
      "                                    '  CNN-Inference-Engine-Quick-View\\n'\n",
      "                                    'A quick view of high-performance '\n",
      "                                    'convolution neural networks (CNNs) '\n",
      "                                    'inference engines on mobile devices. '\n",
      "                                    'Runtime-speed Comparisons\\n'\n",
      "                                    '\\n'\n",
      "                                    ' AI-Benchmarks \\n'\n",
      "                                    '\\n'\n",
      "                                    'Data-flow / Graph Optimization\\n'\n",
      "                                    '\\n'\n",
      "                                    ' nnfusion \\n'\n",
      "                                    ' TASO \\n'\n",
      "                                    ' AMDMIGraphX \\n'\n",
      "                                    '\\n'\n",
      "                                    'FLOAT32-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Detection-Support\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Bolt \\n'\n",
      "                                    'CPU (ARM optimized) / x86 / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali Adreno Apple '\n",
      "                                    'GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' PPLNN \\n'\n",
      "                                    'CPU (ARM/x86 optimized) / Nvidia GPU\\n'\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link / Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Paddle-Light \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / FPGA / '\n",
      "                                    'NPU \\n'\n",
      "                                    'Paddle / Caffe / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' MNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' NCNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU\\n'\n",
      "                                    'Caffe / PyTorch / mxnet / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' 3rd party Link / Official Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' MACE \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / DSP\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TEngine \\n'\n",
      "                                    'CPU (ARM A72 optimized)\\n'\n",
      "                                    'Caffe / mxnet\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' AutoKernel \\n'\n",
      "                                    'CPU / GPU/ NPU\\n'\n",
      "                                    'Caffe / mxnet / Tensorflow / PyTorch / '\n",
      "                                    'Darknet\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Synet \\n'\n",
      "                                    'CPU (ARM optimized) / x86\\n'\n",
      "                                    'Caffe / PyTorch / Tensorflow / mxnet / '\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' MsnhNet \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / x86 / '\n",
      "                                    'TensorRT\\n'\n",
      "                                    'PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' ONNX-Runtime \\n'\n",
      "                                    'CPU / Nvidia GPU\\n'\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' HiAI \\n'\n",
      "                                    'Kirin CPU / NPU\\n'\n",
      "                                    'Caffe / Tensorflow\\n'\n",
      "                                    'Y\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' NNIE \\n'\n",
      "                                    'NPU\\n'\n",
      "                                    'Caffe\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' 1TOPs \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Intel-Caffe \\n'\n",
      "                                    'CPU (Intel optimized)\\n'\n",
      "                                    'Caffe\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' FeatherCNN \\n'\n",
      "                                    'CPU (ARM optimized)\\n'\n",
      "                                    'Caffe\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link / unofficial Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Tensorflowlite \\n'\n",
      "                                    'CPU (Android optimized)\\n'\n",
      "                                    'Caffe2 / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TensorRT \\n'\n",
      "                                    'GPU (Volta optimized)\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TVM \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / FPGA\\n'\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' SNPE \\n'\n",
      "                                    'CPU (Qualcomm optimized) / GPU / DSP\\n'\n",
      "                                    'Caffe / Caffe2 / Tensorflow/ onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Pocket-Tensor \\n'\n",
      "                                    'CPU (ARM/x86 optimized)\\n'\n",
      "                                    'Keras\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' ZQCNN \\n'\n",
      "                                    'CPU\\n'\n",
      "                                    'Caffe / mxnet\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' ARM-NEON-to-x86-SSE \\n'\n",
      "                                    'CPU (Intel optimized)\\n'\n",
      "                                    'Intrinsics-Level\\n'\n",
      "                                    '-\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Simd \\n'\n",
      "                                    'CPU (all platform optimized)\\n'\n",
      "                                    'Intrinsics-Level\\n'\n",
      "                                    '-\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' clDNN \\n'\n",
      "                                    'Intel® Processor Graphics / Iris™ Pro '\n",
      "                                    'Graphics\\n'\n",
      "                                    'Caffe / Tennsorflow / mxnet / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'FIX16-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Detection-Support\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Bolt \\n'\n",
      "                                    'CPU (ARM optimized) / x86 / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' ARM32-SGEMM-LIB \\n'\n",
      "                                    'CPU (ARM optimized)\\n'\n",
      "                                    'GEMM Library\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali Adreno Apple '\n",
      "                                    'GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Yolov2-Xilinx-PYNQ \\n'\n",
      "                                    'FPGA (Xilinx PYNQ)\\n'\n",
      "                                    'Yolov2-only\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'INT8-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Detection-Support\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Bolt \\n'\n",
      "                                    'CPU (ARM optimized) / x86 / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Intel-Caffe \\n'\n",
      "                                    'CPU (Intel Skylake)\\n'\n",
      "                                    'Caffe\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali Adreno Apple '\n",
      "                                    'GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' PPLNN \\n'\n",
      "                                    'Nvidia GPU optimized\\n'\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' NCNN \\n'\n",
      "                                    'CPU (ARM optimized)\\n'\n",
      "                                    'Caffe / pytorch / mxnet / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Paddle-Light \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / FPGA\\n'\n",
      "                                    'Paddle / Caffe / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' MNN \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Tensorflowlite \\n'\n",
      "                                    'CPU (ARM)\\n'\n",
      "                                    'Caffe2 / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TensorRT \\n'\n",
      "                                    'GPU (Volta)\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Gemmlowp \\n'\n",
      "                                    'CPU (ARM / x86)\\n'\n",
      "                                    'GEMM Library\\n'\n",
      "                                    '-\\n'\n",
      "                                    '-\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' SNPE \\n'\n",
      "                                    'DSP (Quantized DLC)\\n'\n",
      "                                    'Caffe / Caffe2 / Tensorflow/ onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' MACE \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / DSP\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TF2 \\n'\n",
      "                                    'FPGA\\n'\n",
      "                                    'Caffe / PyTorch / Tensorflow\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TVM \\n'\n",
      "                                    'CPU (ARM optimized) / Mali GPU / FPGA\\n'\n",
      "                                    'onnx\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'TERNARY-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Detection-Support\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Gemmbitserial \\n'\n",
      "                                    'CPU (ARM / x86)\\n'\n",
      "                                    'GEMM Library\\n'\n",
      "                                    '-\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'BINARY-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Detection-Support\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Bolt \\n'\n",
      "                                    'CPU (ARM optimized) / x86 / Mali GPU\\n'\n",
      "                                    'Caffe / Tensorflow / PyTorch\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' BMXNET \\n'\n",
      "                                    'CPU (ARM / x86) / GPU\\n'\n",
      "                                    'mxnet\\n'\n",
      "                                    'Y\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' DABNN \\n'\n",
      "                                    'CPU (ARM)\\n'\n",
      "                                    'Caffe / Tensorflow / onnx\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Espresso \\n'\n",
      "                                    'GPU\\n'\n",
      "                                    '-\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' BNN-PYNQ \\n'\n",
      "                                    'FPGA (Xilinx PYNQ)\\n'\n",
      "                                    '-\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' FINN \\n'\n",
      "                                    'FPGA (Xilinx)\\n'\n",
      "                                    '-\\n'\n",
      "                                    'N\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'NLP-Support\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    'Framework\\n'\n",
      "                                    'Main Platform\\n'\n",
      "                                    'Model Compatibility\\n'\n",
      "                                    'Speed Benchmarks\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' TurboTransformers \\n'\n",
      "                                    'CPU / Nvidia GPU\\n'\n",
      "                                    'PyTorch\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' Bolt \\n'\n",
      "                                    'CPU / Mali GPU\\n'\n",
      "                                    'Caffe / onnx\\n'\n",
      "                                    ' Link \\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    '\\n'\n",
      "                                    ' ',\n",
      "                         'score': -0.0029170191493903984,\n",
      "                         'title': 'GitHub - '\n",
      "                                  'HolmesShuan/CNN-Inference-Engine-Quick-View: '\n",
      "                                  'A quick view of high-performance '\n",
      "                                  'convolution neural networks (CNNs) '\n",
      "                                  'inference engines on mobile devices.',\n",
      "                         'url': 'https://github.com/HolmesShuan/CNN-Inference-Engine-Quick-View'}]}\n"
     ]
    }
   ],
   "source": [
    "response = superpowered.query_knowledge_bases(\n",
    "    query='What is the latest research on processing units to improve inference time in deep learning? Please respond in json where the keys are `question` and `answer`.',\n",
    "    use_web_search=True,\n",
    "    web_search_preset_id=wsp['id'],\n",
    "    summarize_results=True,\n",
    "    json_response=True,\n",
    ")\n",
    "pprint(json.loads(response['summary']))\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Endpoint\n",
    "\n",
    "Just use thread overrides instead of adding the web search config to the chat thread default options.\n",
    "\n",
    "Using the new `mistral-large` model (released 2024-02-26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'interaction': {'id': '0000000001',\n",
      "                 'model_response': {'content': 'The latest research on '\n",
      "                                               'processing units to improve '\n",
      "                                               'inference time in deep '\n",
      "                                               'learning is quite extensive '\n",
      "                                               'and multifaceted. Here are '\n",
      "                                               'some key findings from the '\n",
      "                                               'papers you provided:\\n'\n",
      "                                               '\\n'\n",
      "                                               '1. **Edge TPUs**: A study by '\n",
      "                                               'Google researchers evaluated '\n",
      "                                               'Edge TPUs, a domain of '\n",
      "                                               'accelerators for low-power, '\n",
      "                                               'edge devices used in various '\n",
      "                                               'Google products. The study '\n",
      "                                               'found that Edge TPU '\n",
      "                                               'accelerators perform '\n",
      "                                               'differently across '\n",
      "                                               'convolutional neural networks '\n",
      "                                               'with different structures. The '\n",
      "                                               'researchers are also '\n",
      "                                               'developing high-accuracy '\n",
      "                                               'learned machine learning '\n",
      "                                               'models to estimate major '\n",
      "                                               'performance metrics of '\n",
      "                                               'accelerators such as latency '\n",
      "                                               'and energy consumption, '\n",
      "                                               'enabling significantly faster '\n",
      "                                               'evaluations of accelerators as '\n",
      "                                               'an alternative to '\n",
      "                                               'time-consuming cycle-accurate '\n",
      "                                               'simulators (Source: \"An '\n",
      "                                               'Evaluation of Edge TPU '\n",
      "                                               'Accelerators for Convolutional '\n",
      "                                               'Neural Networks\").\\n'\n",
      "                                               '\\n'\n",
      "                                               '2. **CPUs**: A paper titled '\n",
      "                                               '\"Optimizing Inference '\n",
      "                                               'Performance of Transformers on '\n",
      "                                               'CPUs\" proposed an optimization '\n",
      "                                               'for the Linear module, where '\n",
      "                                               'each module is augmented with '\n",
      "                                               'a transposeFlags array. This '\n",
      "                                               'array specifies whether to use '\n",
      "                                               'a transposed version of the '\n",
      "                                               'weights matrix for the forward '\n",
      "                                               'pass (inference). The '\n",
      "                                               'optimization allows for a '\n",
      "                                               'reduction in the number of '\n",
      "                                               'profiling and an improvement '\n",
      "                                               'in BERT-base inference '\n",
      "                                               'latency.\\n'\n",
      "                                               '\\n'\n",
      "                                               '3. **Preprocessing '\n",
      "                                               'Pipelines**: Research in the '\n",
      "                                               'paper \"Where Is My Training '\n",
      "                                               'Bottleneck? Hidden Trade-Offs '\n",
      "                                               'in Deep Learning Preprocessing '\n",
      "                                               'Pipelines\" showed that by '\n",
      "                                               'applying generated insights to '\n",
      "                                               'real-world use-cases, an '\n",
      "                                               'increased throughput of 3x to '\n",
      "                                               '13x can be achieved compared '\n",
      "                                               'to an untuned system. The '\n",
      "                                               'researchers also provided an '\n",
      "                                               'open-source profiling library '\n",
      "                                               'that can automatically decide '\n",
      "                                               'on a suitable preprocessing '\n",
      "                                               'strategy to maximize '\n",
      "                                               'throughput.\\n'\n",
      "                                               '\\n'\n",
      "                                               '4. **Dynamic Vision '\n",
      "                                               'Transformers**: A study titled '\n",
      "                                               '\"Enabling and Accelerating '\n",
      "                                               'Dynamic Vision Transformer '\n",
      "                                               'Inference for Real-Time '\n",
      "                                               'Applications\" showed that '\n",
      "                                               'pretrained models are fairly '\n",
      "                                               'resilient to skipping '\n",
      "                                               'computation in the convolution '\n",
      "                                               'and self-attention layers. '\n",
      "                                               'This enables the creation of a '\n",
      "                                               'low-overhead system for '\n",
      "                                               'dynamic real-time inference '\n",
      "                                               'without additional training. '\n",
      "                                               'The researchers also created '\n",
      "                                               'an optimized accelerator for '\n",
      "                                               'these dynamic vision '\n",
      "                                               'transformers in a 5nm '\n",
      "                                               'technology.\\n'\n",
      "                                               '\\n'\n",
      "                                               '5. **Adaptive Inference**: A '\n",
      "                                               'paper titled \"Adaptive '\n",
      "                                               'Inference: Theoretical Limits '\n",
      "                                               'and Unexplored Opportunities\" '\n",
      "                                               'introduced a theoretical '\n",
      "                                               'framework for quantifying the '\n",
      "                                               'efficiency and performance '\n",
      "                                               'gain opportunity size of '\n",
      "                                               'adaptive inference algorithms. '\n",
      "                                               'The researchers provided new '\n",
      "                                               'approximate and exact bounds '\n",
      "                                               'for the achievable efficiency '\n",
      "                                               'and performance gains, '\n",
      "                                               'demonstrating the potential '\n",
      "                                               'for 10-100x efficiency '\n",
      "                                               'improvements in both Computer '\n",
      "                                               'Vision and Natural Language '\n",
      "                                               'Processing tasks without '\n",
      "                                               'incurring any performance '\n",
      "                                               'penalties.\\n'\n",
      "                                               '\\n'\n",
      "                                               'These studies show that there '\n",
      "                                               'are significant advancements '\n",
      "                                               'and potential for improvement '\n",
      "                                               'in the field of processing '\n",
      "                                               'units for deep learning '\n",
      "                                               'inference.',\n",
      "                                    'timestamp': 1709153012},\n",
      "                 'ranked_results': [],\n",
      "                 'references': [],\n",
      "                 'search_queries': None,\n",
      "                 'user_input': {'content': 'What is the latest research on '\n",
      "                                           'processing units to improve '\n",
      "                                           'inference time in deep learning?',\n",
      "                                'timestamp': 1709152990},\n",
      "                 'web_search_queries': [{'query': 'Latest research on '\n",
      "                                                  'processing units for '\n",
      "                                                  'improving inference time in '\n",
      "                                                  'deep learning',\n",
      "                                         'web_search_preset': {'end_date': None,\n",
      "                                                               'exclude_domains': None,\n",
      "                                                               'include_domains': ['arxiv.org',\n",
      "                                                                                   'sciencedirect.com',\n",
      "                                                                                   'wikipedia.org'],\n",
      "                                                               'start_date': None,\n",
      "                                                               'timeframe_days': None}}],\n",
      "                 'web_search_references': [0, 1, 2, 3, 4],\n",
      "                 'web_search_results': [{'content': '{ayazdan, kiransesh, '\n",
      "                                                    'bakin, jlaudon, '\n",
      "                                                    'swamiravi} @ google.com '\n",
      "                                                    'Abstract— Edge TPUs are a '\n",
      "                                                    'domain of accelerators '\n",
      "                                                    'for low-power, edge '\n",
      "                                                    'devices and are widely '\n",
      "                                                    'used in various Google '\n",
      "                                                    'products such as Coral '\n",
      "                                                    'and Pixel devices. In '\n",
      "                                                    'this paper, we first '\n",
      "                                                    'discuss the ma- jor '\n",
      "                                                    'microarchitectural '\n",
      "                                                    'details of Edge TPUs. '\n",
      "                                                    'Then, we extensively '\n",
      "                                                    'evaluate three classes of '\n",
      "                                                    'Edge TPUs, covering '\n",
      "                                                    'different computing '\n",
      "                                                    'ecosystems, that are '\n",
      "                                                    'either currently deployed '\n",
      "                                                    'in Google products or are '\n",
      "                                                    'the product pipeline, '\n",
      "                                                    'across 423K unique '\n",
      "                                                    'convolutional neural '\n",
      "                                                    'networks. Building upon '\n",
      "                                                    'this extensive study, we '\n",
      "                                                    'discuss critical and '\n",
      "                                                    'interpretable '\n",
      "                                                    'microarchitectural '\n",
      "                                                    'insights about the '\n",
      "                                                    'studied classes of Edge '\n",
      "                                                    'TPUs. Mainly, we discuss '\n",
      "                                                    'how Edge TPU accelerators '\n",
      "                                                    'perform across '\n",
      "                                                    'convolutional neural '\n",
      "                                                    'networks with different '\n",
      "                                                    'structures. Finally, we '\n",
      "                                                    'present our ongoing '\n",
      "                                                    'efforts in developing '\n",
      "                                                    'high-accuracy learned '\n",
      "                                                    'machine learning models '\n",
      "                                                    'to estimate the major '\n",
      "                                                    'performance metrics of '\n",
      "                                                    'accelerators such as '\n",
      "                                                    'latency and energy '\n",
      "                                                    'consumption. These '\n",
      "                                                    'learned models enable '\n",
      "                                                    'significantly faster (in '\n",
      "                                                    'the order of '\n",
      "                                                    'milliseconds) evaluations '\n",
      "                                                    'of accelerators as an '\n",
      "                                                    'alternative to '\n",
      "                                                    'time-consuming '\n",
      "                                                    'cycle-accurate simulators '\n",
      "                                                    'and establish an exciting '\n",
      "                                                    'opportunity for rapid '\n",
      "                                                    'hard-',\n",
      "                                         'score': 0.07086961163440719,\n",
      "                                         'title': 'An Evaluation of Edge TPU '\n",
      "                                                  'Accelerators for '\n",
      "                                                  'Convolutional Neural '\n",
      "                                                  'Networks',\n",
      "                                         'url': 'https://arxiv.org/pdf/2102.10423v1.pdf'},\n",
      "                                        {'content': 'One may wonder about the '\n",
      "                                                    'reason for this '\n",
      "                                                    'performance difference. '\n",
      "                                                    'In oneDNN, much like in '\n",
      "                                                    'any math library for '\n",
      "                                                    'high-performance matmul '\n",
      "                                                    'calculation  , the matmul '\n",
      "                                                    'operation is coded in '\n",
      "                                                    'assembly, and each of the '\n",
      "                                                    'matmul variants (e.g., '\n",
      "                                                    'one with both source '\n",
      "                                                    'matrices in the normal '\n",
      "                                                    'form vs. one in which the '\n",
      "                                                    'second matrix is '\n",
      "                                                    'transposed) results in a '\n",
      "                                                    'different code path, '\n",
      "                                                    'which generates different '\n",
      "                                                    'memory access patterns. '\n",
      "                                                    'Based on the profiling '\n",
      "                                                    'information produced by '\n",
      "                                                    'perf, we observe that '\n",
      "                                                    'given a certain '\n",
      "                                                    'configuration (i.e., the '\n",
      "                                                    'same source matrix shapes '\n",
      "                                                    'and the number of '\n",
      "                                                    'threads), both variants '\n",
      "                                                    'have a similar number of '\n",
      "                                                    'L1 data cache misses, but '\n",
      "                                                    'the faster variant has a '\n",
      "                                                    'lower number of L3 cache '\n",
      "                                                    'accesses. This suggests '\n",
      "                                                    'that one reason for '\n",
      "                                                    'performance difference '\n",
      "                                                    'might be the better '\n",
      "                                                    'utilization of L2 cache '\n",
      "                                                    'by one variant over the '\n",
      "                                                    'other. Given the results '\n",
      "                                                    'in Figure 3 , we propose '\n",
      "                                                    'the following '\n",
      "                                                    'optimization for the '\n",
      "                                                    'Linear module. Each '\n",
      "                                                    'Linear module is '\n",
      "                                                    'augmented with a '\n",
      "                                                    'transposeFlags array, '\n",
      "                                                    'specifying whether to use '\n",
      "                                                    'a transposed version of '\n",
      "                                                    'the weights matrix for '\n",
      "                                                    'the forward pass '\n",
      "                                                    '(inference). Entry 𝑖 of '\n",
      "                                                    'the array corresponds to '\n",
      "                                                    'the sequence length of 2 '\n",
      "                                                    '𝑖 ; the array has 10 '\n",
      "                                                    'entries corresponding to '\n",
      "                                                    'the maximal length of 512 '\n",
      "                                                    'tokens. When creating a '\n",
      "                                                    'Linear module with the '\n",
      "                                                    'given weights shape [𝑖𝑛, '\n",
      "                                                    '𝑜𝑢𝑡], we generate random '\n",
      "                                                    'matrices with the shape '\n",
      "                                                    '[2 𝑖 , 𝑖𝑛], for each 0 ≤ '\n",
      "                                                    '𝑖 < 10, and measure the '\n",
      "                                                    'time to perform a matmul '\n",
      "                                                    'operation when the weight '\n",
      "                                                    'matrix is transposed or '\n",
      "                                                    'not. Based on the result, '\n",
      "                                                    'we set the corresponding '\n",
      "                                                    'entry transposeFlags[i]. '\n",
      "                                                    'During the inference '\n",
      "                                                    'time, given the input of '\n",
      "                                                    'shape [𝑙𝑒𝑛𝑔𝑡ℎ, 𝑖𝑛], we '\n",
      "                                                    'calculate 𝑠 = '\n",
      "                                                    '⌊log(𝑙𝑒𝑛𝑔𝑡ℎ)⌋, and based '\n",
      "                                                    'on the flag in '\n",
      "                                                    'transposeFlags[s], '\n",
      "                                                    'perform the matmul '\n",
      "                                                    'operation with either '\n",
      "                                                    'weight matrix transposed '\n",
      "                                                    'or not. To avoid the '\n",
      "                                                    'overhead of transposing '\n",
      "                                                    'the weight matrix during '\n",
      "                                                    'inference, we keep both '\n",
      "                                                    'variants of the weight '\n",
      "                                                    'matrix (transposed and '\n",
      "                                                    'non-transposed one). This '\n",
      "                                                    'doubles the memory '\n",
      "                                                    'footprint of the Linear '\n",
      "                                                    'module. While it might '\n",
      "                                                    'not be a concern on some '\n",
      "                                                    'CPU deployments, there '\n",
      "                                                    'are several ways to '\n",
      "                                                    'mitigate this drawback. '\n",
      "                                                    'First, some shapes always '\n",
      "                                                    'prefer one form over the '\n",
      "                                                    'other, for all thread '\n",
      "                                                    'counts (e.g., the shape '\n",
      "                                                    '3072-768 in Figure 3 '\n",
      "                                                    '(a)-(d) ). For this case, '\n",
      "                                                    'we can keep only the '\n",
      "                                                    'relevant variant of the '\n",
      "                                                    'weight matrix. Second, '\n",
      "                                                    'the length of the input '\n",
      "                                                    'can be known prior to the '\n",
      "                                                    'deployment of an '\n",
      "                                                    'inference server, e.g., '\n",
      "                                                    'in a farm of inference '\n",
      "                                                    'servers, certain servers '\n",
      "                                                    'can be configured to '\n",
      "                                                    'handle input of a certain '\n",
      "                                                    'sequence length. Once '\n",
      "                                                    'again, in this case we '\n",
      "                                                    'can keep only the '\n",
      "                                                    'relevant variant of the '\n",
      "                                                    'weight matrix. Finally, '\n",
      "                                                    'if the input range is '\n",
      "                                                    'dynamic, one can store '\n",
      "                                                    'one variant of the weight '\n",
      "                                                    'matrix and transpose '\n",
      "                                                    'on-demand. The selection '\n",
      "                                                    'of the stored variant can '\n",
      "                                                    'be also dynamic and tuned '\n",
      "                                                    'based on the actual input '\n",
      "                                                    'lengths seen during the '\n",
      "                                                    'runtime. All those '\n",
      "                                                    'mitigation ideas are left '\n",
      "                                                    'for the future work. We '\n",
      "                                                    'note that transposeFlags '\n",
      "                                                    'can be shared among '\n",
      "                                                    'Linear modules of the '\n",
      "                                                    'same shape. We use a '\n",
      "                                                    'key-value map '\n",
      "                                                    '(dictionary) to store '\n",
      "                                                    'transposeFlags arrays '\n",
      "                                                    'where the [𝑖𝑛, 𝑜𝑢𝑡] tuple '\n",
      "                                                    'of corresponding Linear '\n",
      "                                                    'modules serves as a key. '\n",
      "                                                    'Thus, when initializing '\n",
      "                                                    'the transposeFlags array, '\n",
      "                                                    'we query the dictionary '\n",
      "                                                    'first, and if such a '\n",
      "                                                    'shape has been already '\n",
      "                                                    'profiled, we reuse the '\n",
      "                                                    'resulting array, skipping '\n",
      "                                                    'the profiling phase for '\n",
      "                                                    'that Linear module. For '\n",
      "                                                    'the BERT model, this '\n",
      "                                                    'optimization allows us to '\n",
      "                                                    'reduce the number of '\n",
      "                                                    'profiling 1 : BERT-base '\n",
      "                                                    'inference latency (ms), '\n",
      "                                                    'for various sequence '\n",
      "                                                    'lengths. The numbers in '\n",
      "                                                    '() show the speedup of '\n",
      "                                                    'onednn-almo over '\n",
      "                                                    'onednn-base.',\n",
      "                                         'score': 0.05864171689859648,\n",
      "                                         'title': 'Optimizing Inference '\n",
      "                                                  'Performance of Transformers '\n",
      "                                                  'on CPUs',\n",
      "                                         'url': 'https://arxiv.org/abs/2102.06621v1'},\n",
      "                                        {'content': 'preprocessing time, and '\n",
      "                                                    'storage consumption. '\n",
      "                                                    'Additionally, we provide '\n",
      "                                                    'an open-source profiling '\n",
      "                                                    'library that can '\n",
      "                                                    'automatically decide on a '\n",
      "                                                    'suitable preprocessing '\n",
      "                                                    'strategy to maximize '\n",
      "                                                    'throughput. By applying '\n",
      "                                                    'our generated insights to '\n",
      "                                                    'real-world use-cases, we '\n",
      "                                                    'obtain an increased '\n",
      "                                                    'throughput of 3x to 13x '\n",
      "                                                    'compared to an untuned '\n",
      "                                                    'system while keeping the '\n",
      "                                                    'pipeline functionally '\n",
      "                                                    'identical. These findings '\n",
      "                                                    'show the enormous '\n",
      "                                                    'potential of data '\n",
      "                                                    'pipeline tuning.  '\n",
      "                                                    '<h2>Submission '\n",
      "                                                    'history</h2><p> From: '\n",
      "                                                    'Alexander Isenko [<a '\n",
      "                                                    'href=\"/show-email/3360e0b6/2202.08679\">view '\n",
      "                                                    'email</a>]  <strong><a '\n",
      "                                                    'href=\"/abs/2202.08679v1\">[v1]</a></strong>  '\n",
      "                                                    '<strong><a '\n",
      "                                                    'href=\"/abs/2202.08679v2\">[v2]</a></strong>  '\n",
      "                                                    'Fri, 25 Feb 2022 14:35:44 '\n",
      "                                                    'UTC (22,625 KB)<br '\n",
      "                                                    '/><strong>[v3]</strong> '\n",
      "                                                    'Fri, 25 Mar 2022 09:54:55 '\n",
      "                                                    'UTC (22,658 KB)<br '\n",
      "                                                    '/></p></div>',\n",
      "                                         'score': 0.05360836298495997,\n",
      "                                         'title': 'Where Is My Training '\n",
      "                                                  'Bottleneck? Hidden '\n",
      "                                                  'Trade-Offs in Deep Learning '\n",
      "                                                  'Preprocessing Pipelines',\n",
      "                                         'url': 'https://arxiv.org/abs/2202.08679'},\n",
      "                                        {'content': 'We show that pretrained '\n",
      "                                                    'models are fairly '\n",
      "                                                    'resilient to skipping '\n",
      "                                                    'computation in the '\n",
      "                                                    'convolution and '\n",
      "                                                    'self-attention layers, '\n",
      "                                                    'enabling us to create a '\n",
      "                                                    'low-overhead system for '\n",
      "                                                    'dynamic real-time '\n",
      "                                                    'inference without '\n",
      "                                                    'additional training. '\n",
      "                                                    'Finally, we create a '\n",
      "                                                    'optimized accelerator for '\n",
      "                                                    'these dynamic vision '\n",
      "                                                    'transformers in a 5nm '\n",
      "                                                    'technology. The PE array '\n",
      "                                                    'occupies 2.26mm$^2$ and '\n",
      "                                                    'is 17 times faster than a '\n",
      "                                                    'NVIDIA TITAN V GPU for '\n",
      "                                                    'state-of-the-art '\n",
      "                                                    'transformer-based models '\n",
      "                                                    'for semantic  '\n",
      "                                                    '<h2>Submission '\n",
      "                                                    'history</h2><p> From: '\n",
      "                                                    'Kavya Sreedhar [<a '\n",
      "                                                    'href=\"/show-email/d79bb569/2212.02687\">view '\n",
      "                                                    'email</a>] Tue, 6 Dec '\n",
      "                                                    '2022 01:10:31 UTC (6,467 '\n",
      "                                                    'KB)<br /></p></div>',\n",
      "                                         'score': 0.02091353153809905,\n",
      "                                         'title': 'Enabling and Accelerating '\n",
      "                                                  'Dynamic Vision Transformer '\n",
      "                                                  'Inference for Real-Time '\n",
      "                                                  'Applications',\n",
      "                                         'url': 'https://arxiv.org/abs/2212.02687'},\n",
      "                                        {'content': '<p><a '\n",
      "                                                    'href=\"https://arxiv.org/pdf/2402.04359.pdf\">Download '\n",
      "                                                    'PDF</a></p><blockquote> '\n",
      "                                                    'This paper introduces the '\n",
      "                                                    'first theoretical '\n",
      "                                                    'framework for quantifying '\n",
      "                                                    'the efficiency and '\n",
      "                                                    'performance gain '\n",
      "                                                    'opportunity size of '\n",
      "                                                    'adaptive inference '\n",
      "                                                    'algorithms. We provide '\n",
      "                                                    'new approximate and exact '\n",
      "                                                    'bounds for the achievable '\n",
      "                                                    'efficiency and '\n",
      "                                                    'performance gains, '\n",
      "                                                    'supported by empirical '\n",
      "                                                    'evidence demonstrating '\n",
      "                                                    'the potential for 10-100x '\n",
      "                                                    'efficiency improvements '\n",
      "                                                    'in both Computer Vision '\n",
      "                                                    'and Natural Language '\n",
      "                                                    'Processing tasks without '\n",
      "                                                    'incurring any performance '\n",
      "                                                    'penalties. Additionally, '\n",
      "                                                    'we offer insights on '\n",
      "                                                    'improving achievable '\n",
      "                                                    'efficiency gains through '\n",
      "                                                    'the optimal selection and '\n",
      "                                                    'design of adaptive '\n",
      "                                                    'inference state spaces. '\n",
      "                                                    '<h2>Submission '\n",
      "                                                    'history</h2><p> From: '\n",
      "                                                    'Soheil Hor [<a '\n",
      "                                                    'href=\"https://arxiv.org/show-email/93491051/2402.04359\">view '\n",
      "                                                    'email</a>] <br/> '\n",
      "                                                    '<strong>[v1]</strong>',\n",
      "                                         'score': -0.0059155588842986625,\n",
      "                                         'title': 'Adaptive Inference: '\n",
      "                                                  'Theoretical Limits and '\n",
      "                                                  'Unexplored Opportunities',\n",
      "                                         'url': 'https://arxiv.org/abs/2402.04359'}]},\n",
      " 'ranked_results': [],\n",
      " 'search_queries': None}\n"
     ]
    }
   ],
   "source": [
    "thread = superpowered.create_chat_thread()\n",
    "\n",
    "response = superpowered.get_chat_response(\n",
    "    thread_id=thread['id'],\n",
    "    input='What is the latest research on processing units to improve inference time in deep learning?',\n",
    "    model='mistral-large',\n",
    "    use_web_search=True,\n",
    "    web_search_include_domains=['arxiv.org', 'sciencedirect.com', 'wikipedia.org'],\n",
    ")\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
